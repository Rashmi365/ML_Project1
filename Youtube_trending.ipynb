{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX5UnySxEfJn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "from collections import Counter\n",
        "import datetime\n",
        "import wordcloud\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hiding warnings for cleaner display\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuring some options\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "# If you want interactive plots, uncomment the next line\n",
        "# %matplotlib notebook"
      ],
      "metadata": {
        "id": "uum80S6BFgjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import the dataset\n"
      ],
      "metadata": {
        "id": "2WneHgk0FkgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"../input/USvideos.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "zdL8VD9XFrf0",
        "outputId": "04218b20-50d9-4db6-e31b-6e12b297bdc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e4469b4bcece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kaggle kernels output ammar111/youtube-trending-videos-analysis -p /path/to/dest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read_url'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set some configuration options just for improving visualization graphs; nothing crucial"
      ],
      "metadata": {
        "id": "Xk3tafaPF3OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PLOT_COLORS = [\"#268bd2\", \"#0052CC\", \"#FF5722\", \"#b58900\", \"#003f5c\"]\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "sns.set(style=\"ticks\")\n",
        "plt.rc('figure', figsize=(8, 5), dpi=100)\n",
        "plt.rc('axes', labelpad=20, facecolor=\"#ffffff\", linewidth=0.4, grid=True, labelsize=14)\n",
        "plt.rc('patch', linewidth=0)\n",
        "plt.rc('xtick.major', width=0.2)\n",
        "plt.rc('ytick.major', width=0.2)\n",
        "plt.rc('grid', color='#9E9E9E', linewidth=0.4)\n",
        "plt.rc('font', family='Arial', weight='400', size=10)\n",
        "plt.rc('text', color='#282828')\n",
        "plt.rc('savefig', pad_inches=0.3, dpi=300)"
      ],
      "metadata": {
        "id": "DigsmTeZFyCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "Getting a feel of the dataset\n",
        "Let's get a feel of our dataset by displaying its first few rows"
      ],
      "metadata": {
        "id": "T5MMCIG5GA5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df.info()"
      ],
      "metadata": {
        "id": "8QCLJqLzF51m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are 40,949 entries in the dataset. We can see also that all columns in the dataset are complete (i.e. they have 40,949 non-null entries) except description column which has some null values; it only has 40,379 non-null values.\n",
        "\n",
        "Data cleaning\n",
        "The description column has some null values. These are some of the rows whose description values are null. We can see that null values are denoted by NaN"
      ],
      "metadata": {
        "id": "DTJ4a0LxGP45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"description\"].apply(lambda x: pd.isna(x))].head(3)"
      ],
      "metadata": {
        "id": "y8dle4LyGmWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So to do some sort of data cleaning, and to get rid of those null values, we put an empty string in place of each null value in the description column\n"
      ],
      "metadata": {
        "id": "YAPROdaFGrK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"description\"] = df[\"description\"].fillna(value=\"\")"
      ],
      "metadata": {
        "id": "8UzklJg3GyJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset collection years\n",
        "Let's see in which years the dataset was collected"
      ],
      "metadata": {
        "id": "fCTrl-baG2Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cdf = df[\"trending_date\"].apply(lambda x: '20' + x[:2]).value_counts() \\\n",
        "            .to_frame().reset_index() \\\n",
        "            .rename(columns={\"index\": \"year\", \"trending_date\": \"No_of_videos\"})\n",
        "​\n",
        "fig, ax = plt.subplots()\n",
        "_ = sns.barplot(x=\"year\", y=\"No_of_videos\", data=cdf, \n",
        "                palette=sns.color_palette(['#ff764a', '#ffa600'], n_colors=7), ax=ax)\n",
        "_ = ax.set(xlabel=\"Year\", ylabel=\"No. of videos\")\n"
      ],
      "metadata": {
        "id": "RTpaDLRYHCgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"trending_date\"].apply(lambda x: '20' + x[:2]).value_counts(normalize=True)\n"
      ],
      "metadata": {
        "id": "7sw4VFWwHo3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the dataset was collected in 2017 and 2018 with 77% of it in 2018 and 23% in 2017.\n",
        "\n",
        "Describtion of numerical columns\n",
        "Now, let's see some statistical information about the numerical columns of our dataset\n"
      ],
      "metadata": {
        "id": "F5rRowtDHs4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()\n"
      ],
      "metadata": {
        "id": "B5EKMyCEHzD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note from the table above that\n",
        "\n",
        "The average number of views of a trending video is 2,360,784. The median value for the number of views is 681,861, which means that half the trending videos have views that are less than that number, and the other half have views larger than that number\n",
        "The average number of likes of a trending video is 74,266, while the average number of dislikes is 3,711. The\n",
        "Average comment count is 8,446 while the median is 1,856\n",
        "How useful are the observations above? Do they really represent the data? Let's examine more.\n",
        "\n",
        "Views histogram\n",
        "let's plot a histogram for the views column to take a look at its distribution: to see how many videos have between 10 million and 20 million views, how many videos have between 20 million and 30 million views, and so on.\n",
        "\n"
      ],
      "metadata": {
        "id": "TvN646M2ICJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "_ = sns.distplot(df[\"views\"], kde=False, color=PLOT_COLORS[4], \n",
        "                 hist_kws={'alpha': 1}, bins=np.linspace(0, 2.3e8, 47), ax=ax)\n",
        "_ = ax.set(xlabel=\"Views\", ylabel=\"No. of videos\", xticks=np.arange(0, 2.4e8, 1e7))\n",
        "_ = ax.set_xlim(right=2.5e8)\n",
        "_ = plt.xticks(rotation=90)\n"
      ],
      "metadata": {
        "id": "ISw7K9ueIElc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We note that the vast majority of trending videos have 5 million views or less. We get the 5 million number by calculating\n",
        "\n",
        "0.1×1082=5×106\n",
        " \n",
        "Now let us plot the histogram just for videos with 25 million views or less to get a closer look at the distribution of the data\n",
        "\n"
      ],
      "metadata": {
        "id": "MI1Of1aBIJqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "_ = sns.distplot(df[df[\"views\"] < 25e6][\"views\"], kde=False, \n",
        "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, ax=ax)\n",
        "_ = ax.set(xlabel=\"Views\", ylabel=\"No. of videos\")\n"
      ],
      "metadata": {
        "id": "Wj4TNoFzIQll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we see that the majority of trending videos have 1 million views or less. Let's see the exact percentage of videos less than 1 million views\n"
      ],
      "metadata": {
        "id": "i_JH3tQ3IWiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df[df['views'] < 1e6]['views'].count() / df['views'].count() * 100\n"
      ],
      "metadata": {
        "id": "DI_OmWPdGMNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "So, it is around 60%. Similarly, we can see that the percentage of videos with less than 1.5 million views is around 71%, and that the percentage of videos with less than 5 million views is around 91%.\n",
        "\n",
        "Likes histogram\n",
        "After views, we plot the histogram for likes column\n",
        "\n",
        "add Codeadd Markdown\n"
      ],
      "metadata": {
        "id": "eYhYnaKvIvyE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rc('figure.subplot', wspace=0.9)\n",
        "fig, ax = plt.subplots()\n",
        "_ = sns.distplot(df[\"likes\"], kde=False, \n",
        "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, \n",
        "                 bins=np.linspace(0, 6e6, 61), ax=ax)\n",
        "_ = ax.set(xlabel=\"Likes\", ylabel=\"No. of videos\")\n",
        "_ = plt.xticks(rotation=90)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vBTlZZoiI4OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "We note that the vast majority of trending videos have between 0 and 100,000 likes. Let us plot the histogram just for videos with 1000,000 likes or less to get a closer look at the distribution of the data\n",
        "\n",
        "add Codeadd Markdown\n"
      ],
      "metadata": {
        "id": "eRcSz_zcUnFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "_ = sns.distplot(df[df[\"likes\"] <= 1e5][\"likes\"], kde=False, \n",
        "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, ax=ax)\n",
        "_ = ax.set(xlabel=\"Likes\", ylabel=\"No. of videos\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JqFZpKlFUjfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "Now we can see that the majority of trending videos have 40000 likes or less with a peak for videos with 2000 likes or less.\n",
        "\n",
        "Let's see the exact percentage of videos with less than 40000 likes\n",
        "\n",
        "add Codeadd Markdown"
      ],
      "metadata": {
        "id": "AL4L2EzbU7_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['likes'] < 4e4]['likes'].count() / df['likes'].count() * 100\n"
      ],
      "metadata": {
        "id": "Xk3WSFWKUcI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we can see that the percentage of videos with less than 100,000 likes is around 84%\n",
        "\n",
        "Comment count histogram"
      ],
      "metadata": {
        "id": "locdu6-SVB_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "_ = sns.distplot(df[\"comment_count\"], kde=False, rug=False, \n",
        "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, ax=ax)\n",
        "_ = ax.set(xlabel=\"Comment Count\", ylabel=\"No. of videos\")\n",
        "add Codeadd Markdown\n",
        "Let's get a closer look by eliminating entries with comment count larger than 200000 comment\n",
        "\n",
        "add Codeadd Markdown\n",
        "fig, ax = plt.subplots()\n",
        "_ = sns.distplot(df[df[\"comment_count\"] < 200000][\"comment_count\"], kde=False, rug=False, \n",
        "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, \n",
        "                 bins=np.linspace(0, 2e5, 49), ax=ax)\n",
        "_ = ax.set(xlabel=\"Comment Count\", ylabel=\"No. of videos\")\n"
      ],
      "metadata": {
        "id": "JkVKBGKHUOdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "We see that most trending videos have around\n",
        "\n",
        "250006≈4166 comments\n",
        " \n",
        "since each division in the graph has six histogram bins.\n",
        "\n",
        "As with views and likes, let's see the exact percentage of videos with less than 4000 comments\n",
        "\n",
        "add Codeadd Markdown"
      ],
      "metadata": {
        "id": "uHeh-12wUIC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['comment_count'] < 4000]['comment_count'].count() / df['comment_count'].count() * 100\n"
      ],
      "metadata": {
        "id": "ZgJsecNXUCDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "In a similar way, we can see that the percentage of videos with less than 25,000 comments is around 93%.\n",
        "\n",
        "add Codeadd Markdown\n",
        "Description on non-numerical columns\n",
        "After we described numerical columns previously, we now describe non-numerical columns\n",
        "\n",
        "add Codeadd Markdown"
      ],
      "metadata": {
        "id": "tp6pmmbRT-bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include = ['O'])\n"
      ],
      "metadata": {
        "id": "bXZohekbT4Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "From the table above, we can see that there are 205 unique dates, which means that our dataset contains collected data about trending videos over 205 days.\n",
        "\n",
        "From video_id description, we can see that there are 40949 videos (which is expected because our dataset contains 40949 entries), but we can see also that there are only 6351 unique videos which means that some videos appeared on the trending videos list on more than one day. The table also tells us that the top frequent title is WE MADE OUR MOM CRY...HER DREAM CAME TRUE! and that it appeared 30 times on the trending videos list.\n",
        "\n",
        "But there is something strange in the description table above: Because there are 6351 unique video IDs, we expect to have 6351 unique video titles also, because we assume that each ID is linked to a corresponding title. One possible interpretation is that a trending video had some title when it appeared on the trending list, then it appeared again on another day but with a modified title. Similar explaination applies for description column as well. For publish_time column, the unique values are less than 6351, but there is nothing strange here, because two different videos may be published at the same time.\n",
        "\n",
        "To verify our interpretation for title column, let's take a look at an example where a trending video appeared more than once on the trending list but with different titles\n"
      ],
      "metadata": {
        "id": "Sx3eK6JgTptp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "add Codeadd Markdown\n",
        "grouped = df.groupby(\"video_id\")\n",
        "groups = []\n",
        "wanted_groups = []\n",
        "for key, item in grouped:\n",
        "    groups.append(grouped.get_group(key))\n",
        "​\n",
        "for g in groups:\n",
        "    if len(g['title'].unique()) != 1:\n",
        "        wanted_groups.append(g)\n",
        "​\n",
        "wanted_groups[0]"
      ],
      "metadata": {
        "id": "y0JCumGkTebQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "We can see that this video appeared on the list with two different titles.\n",
        "\n",
        "add Codeadd Markdown\n",
        "How many trending video titles contain capitalized word?\n",
        "Now we want to see how many trending video titles contain at least a capitalized word (e.g. HOW). To do that, we will add a new variable (column) to the dataset whose value is True if the video title has at least a capitalized word in it, and False otherwise\n",
        "\n",
        "add Codeadd Markdown\n"
      ],
      "metadata": {
        "id": "kTg79CD1Taqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def contains_capitalized_word(s):\n",
        "    for w in s.split():\n",
        "        if w.isupper():\n",
        "            return True\n",
        "    return False\n",
        "​\n",
        "​\n",
        "df[\"contains_capitalized\"] = df[\"title\"].apply(contains_capitalized_word)\n",
        "​\n",
        "value_counts = df[\"contains_capitalized\"].value_counts().to_dict()\n",
        "fig, ax = plt.subplots()\n",
        "_ = ax.pie([value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n",
        "           colors=['#003f5c', '#ffa600'], textprops={'color': '#040204'}, startangle=45)\n",
        "_ = ax.axis('equal')\n",
        "_ = ax.set_title('Title Contains Capitalized Word?')\n",
        "add Codeadd Markdown\n",
        "df[\"contains_capitalized\"].value_counts(normalize=True)\n",
        "add Codeadd Markdown"
      ],
      "metadata": {
        "id": "piHPz2BYTVzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We can see that 44% of trending video titles contain at least a capitalized word. We will later use this added new column contains_capitalized in analyzing correlation between variables.\n",
        "\n",
        "add Codeadd Markdown\n",
        "Video title lengths\n",
        "Let's add another column to our dataset to represent the length of each video title, then plot the histogram of title length to get an idea about the lengths of trnding video titles\n",
        "\n",
        "add Codeadd Markdown"
      ],
      "metadata": {
        "id": "lMCj4KjPTRMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df[\"title_length\"] = df[\"title\"].apply(lambda x: len(x))\n",
        "​\n",
        "fig, ax = plt.subplots()\n",
        "_ = sns.distplot(df[\"title_length\"], kde=False, rug=False, \n",
        "                 color=PLOT_COLORS[4], hist_kws={'alpha': 1}, ax=ax)\n",
        "_ = ax.set(xlabel=\"Title Length\", ylabel=\"No. of videos\", xticks=range(0, 110, 10))"
      ],
      "metadata": {
        "id": "z-K28OGHTMY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "We can see that title-length distribution resembles a normal distribution, where most videos have title lengths between 30 and 60 character approximately.\n",
        "\n",
        "Now let's draw a scatter plot between title length and number of views to see the relationship between these two variables\n",
        "\n",
        "add Codeadd Markdown\n"
      ],
      "metadata": {
        "id": "faqkCnI6L5x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, ax = plt.subplots()\n",
        "_ = ax.scatter(x=df['views'], y=df['title_length'], color=PLOT_COLORS[2], edgecolors=\"#000000\", linewidths=0.5)\n",
        "_ = ax.set(xlabel=\"Views\", ylabel=\"Title Length\")\n"
      ],
      "metadata": {
        "id": "I0lHHdpdL3lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "By looking at the scatter plot, we can say that there is no relationship between the title length and the number of views. However, we notice an interesting thing: videos that have 100,000,000 views and more have title length between 33 and 55 characters approximately.\n",
        "\n",
        "add Codeadd Markdown\n",
        "Correlation between dataset variables\n",
        "Now let's see how the dataset variables are correlated with each other: for example, we would like to see how views and likes are correlated, meaning do views and likes increase and decrease together (positive correlation)? Does one of them increase when the other decrease and vice versa (negative correlation)? Or are they not correlated?\n",
        "\n",
        "Correlation is represented as a value between -1 and +1 where +1 denotes the highest positive correlation, -1 denotes the highest negative correlation, and 0 denotes that there is no correlation.\n",
        "\n",
        "Let's see the correlation table between our dataset variables (numerical and boolean variables only)\n",
        "\n"
      ],
      "metadata": {
        "id": "n-BkZCWILzjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()\n"
      ],
      "metadata": {
        "id": "ivk7DQ9MLv05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see for example that views and likes are highly positively correlated with a correlation value of 0.85; we see also a high positive correlation (0.80) between likes and comment count, and between dislikes and comment count (0.70).\n",
        "\n",
        "There is some positive correlation between views and dislikes, between views and comment count, between likes and dislikes.\n",
        "\n",
        "Now let's visualize the correlation table above using a heatmap\n",
        "\n",
        "add Codeadd Markdown\n"
      ],
      "metadata": {
        "id": "n1Dz78DELMwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h_labels = [x.replace('_', ' ').title() for x in \n",
        "            list(df.select_dtypes(include=['number', 'bool']).columns.values)]\n",
        "​\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "_ = sns.heatmap(df.corr(), annot=True, xticklabels=h_labels, yticklabels=h_labels, cmap=sns.cubehelix_palette(as_cmap=True), ax=ax)\n"
      ],
      "metadata": {
        "id": "bhNuBaBVLJNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation map and correlation table above say that views and likes are highly positively correlated. Let's verify that by plotting a scatter plot between views and likes to visualize the relationship between these variables\n",
        "\n"
      ],
      "metadata": {
        "id": "NYCvri3PLCpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "_ = plt.scatter(x=df['views'], y=df['likes'], color=PLOT_COLORS[2], edgecolors=\"#000000\", linewidths=0.5)\n",
        "_ = ax.set(xlabel=\"Views\", ylabel=\"Likes\")\n"
      ],
      "metadata": {
        "id": "EyzurVrKLASV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that views and likes are truly positively correlated: as one increases, the other increases too—mostly.\n",
        "\n",
        "Another verification of the correlation matrix and map is the scatter plot we drew above between views and title length as it shows that there is no correlation between them.\n",
        "\n",
        "add Codeadd Markdown\n",
        "Most common words in video titles\n",
        "Let's see if there are some words that are used significantly in trending video titles. We will display the 25 most common words in all trending video titles\n",
        "\n",
        "add Codeadd Markdown\n"
      ],
      "metadata": {
        "id": "ugPCGG5IK89H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title_words = list(df[\"title\"].apply(lambda x: x.split()))\n",
        "title_words = [x for y in title_words for x in y]\n",
        "Counter(title_words).most_common(25)\n"
      ],
      "metadata": {
        "id": "NXIeQOvPK5jJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "Ignoring words like \"the\" and \"of\", we can see that \"-\" and \"|\" symbols occured a lot in the 40949 trending video titles: 11452 times and 10663 times respectively. We notice also that words \"Video\", \"Trailer\", \"How\", and \"2018\" are common in trending video titles; each occured in 1613-1901 video titles.\n",
        "\n",
        "Let's draw a word cloud for the titles of our trending videos, which is a way to visualize most common words in the titles; the more common the word is, the bigger its font size is\n"
      ],
      "metadata": {
        "id": "BYpsuyokK2QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# wc = wordcloud.WordCloud(width=1200, height=600, collocations=False, stopwords=None, background_color=\"white\", colormap=\"tab20b\").generate_from_frequencies(dict(Counter(title_words).most_common(150)))\n",
        "wc = wordcloud.WordCloud(width=1200, height=500, \n",
        "                         collocations=False, background_color=\"white\", \n",
        "                         colormap=\"tab20b\").generate(\" \".join(title_words))\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "_ = plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "sxja7oEkKx4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "add Codeadd Markdown\n",
        "Which channels have the largest number of trending videos?\n",
        "add Codeadd Markdown"
      ],
      "metadata": {
        "id": "PPoimwCnKt8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cdf = df.groupby(\"channel_title\").size().reset_index(name=\"video_count\") \\\n",
        "    .sort_values(\"video_count\", ascending=False).head(20)\n",
        "​\n",
        "fig, ax = plt.subplots(figsize=(8,8))\n",
        "_ = sns.barplot(x=\"video_count\", y=\"channel_title\", data=cdf,\n",
        "                palette=sns.cubehelix_palette(n_colors=20, reverse=True), ax=ax)\n",
        "_ = ax.set(xlabel=\"No. of videos\", ylabel=\"Channel\")"
      ],
      "metadata": {
        "id": "bt3kINGzKrpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Which video category has the largest number of trending videos?\n",
        "First, we will add a column that contains category names based on the values in category_id column. We will use a category JSON file provided with the dataset which contains information about each category.\n",
        "\n"
      ],
      "metadata": {
        "id": "S-GsH7shKl8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"../input/US_category_id.json\") as f:\n",
        "    categories = json.load(f)[\"items\"]\n",
        "cat_dict = {}\n",
        "for cat in categories:\n",
        "    cat_dict[int(cat[\"id\"])] = cat[\"snippet\"][\"title\"]\n",
        "df['category_name'] = df['category_id'].map(cat_dict)\n"
      ],
      "metadata": {
        "id": "IR-WZtbDKh4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see which category had the largest number of trending videos\n"
      ],
      "metadata": {
        "id": "svu6QVTrKcQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cdf = df[\"category_name\"].value_counts().to_frame().reset_index()\n",
        "cdf.rename(columns={\"index\": \"category_name\", \"category_name\": \"No_of_videos\"}, inplace=True)\n",
        "fig, ax = plt.subplots()\n",
        "_ = sns.barplot(x=\"category_name\", y=\"No_of_videos\", data=cdf, \n",
        "                palette=sns.cubehelix_palette(n_colors=16, reverse=True), ax=ax)\n",
        "_ = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "_ = ax.set(xlabel=\"Category\", ylabel=\"No. of videos\")\n"
      ],
      "metadata": {
        "id": "7hpUDBbHKYxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We see that the Entertainment category contains the largest number of trending videos among other categories: around 10,000 videos, followed by Music category with around 6,200 videos, followed by Howto & Style category with around 4,100 videos, and so on.\n",
        "\n",
        "Trending videos and their publishing time\n",
        "An example value of the publish_time column in our dataset is 2017-11-13T17:13:01.000Z. And according to information on this page: https://www.w3.org/TR/NOTE-datetime, this means that the date of publishing the video is 2017-11-13 and the time is 17:13:01 in Coordinated Universal Time (UTC) time zone.\n",
        "\n",
        "Let's add two columns to represent the date and hour of publishing each video, then delete the original publish_time column because we will not need it anymore\n",
        "\n"
      ],
      "metadata": {
        "id": "Ap9nDWOYKSaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"publishing_day\"] = df[\"publish_time\"].apply(\n",
        "    lambda x: datetime.datetime.strptime(x[:10], \"%Y-%m-%d\").date().strftime('%a'))\n",
        "df[\"publishing_hour\"] = df[\"publish_time\"].apply(lambda x: x[11:13])\n",
        "df.drop(labels='publish_time', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "CA2gJUIgKM4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now we can see which days of the week had the largest numbers of trending videos\n",
        "\n",
        "add Codeadd Markdown"
      ],
      "metadata": {
        "id": "IAaf728fKIIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cdf = df[\"publishing_day\"].value_counts()\\\n",
        "        .to_frame().reset_index().rename(columns={\"index\": \"publishing_day\", \"publishing_day\": \"No_of_videos\"})\n",
        "fig, ax = plt.subplots()\n",
        "_ = sns.barplot(x=\"publishing_day\", y=\"No_of_videos\", data=cdf, \n",
        "                palette=sns.color_palette(['#003f5c', '#374c80', '#7a5195', \n",
        "                                           '#bc5090', '#ef5675', '#ff764a', '#ffa600'], n_colors=7), ax=ax)\n",
        "_ = ax.set(xlabel=\"Publishing Day\", ylabel=\"No. of videos\")"
      ],
      "metadata": {
        "id": "qPuKYNDvKEVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the number of trending videos published on Sunday and Saturday are noticeably less than the number of trending videos published on other days of the week.\n",
        "\n",
        "Now let's use publishing_hour column to see which publishing hours had the largest number of trending videos\n"
      ],
      "metadata": {
        "id": "HqvAQsliJ-yZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cdf = df[\"publishing_hour\"].value_counts().to_frame().reset_index()\\\n",
        "        .rename(columns={\"index\": \"publishing_hour\", \"publishing_hour\": \"No_of_videos\"})\n",
        "fig, ax = plt.subplots()\n",
        "_ = sns.barplot(x=\"publishing_hour\", y=\"No_of_videos\", data=cdf, \n",
        "                palette=sns.cubehelix_palette(n_colors=24), ax=ax)\n",
        "_ = ax.set(xlabel=\"Publishing Hour\", ylabel=\"No. of videos\")\n",
        "add Codeadd Markdown"
      ],
      "metadata": {
        "id": "FAIkV9M_J6qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the period between 2PM and 7PM, peaking between 4PM and 5PM, had the largest number of trending videos. We notice also that the period between 12AM and 1PM has the smallest number of trending videos. But why is that? Is it because people publish a lot more videos between 2PM and 7PM? Is it because how YouTube algorithm chooses trending videos?\n",
        "\n",
        "add Codeadd Markdown\n",
        "How many trending videos have an error?\n",
        "To see how many trending videos got removed or had some error, we can use video_error_or_removed column in the dataset\n"
      ],
      "metadata": {
        "id": "9WjC_PxUJ1vJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "add Codeadd Markdown\n",
        "value_counts = df[\"video_error_or_removed\"].value_counts().to_dict()\n",
        "fig, ax = plt.subplots()\n",
        "_ = ax.pie([value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n",
        "        colors=['#003f5c', '#ffa600'], textprops={'color': '#040204'})\n",
        "_ = ax.axis('equal')\n",
        "_ = ax.set_title('Video Error or Removed?')\n",
        "add Codeadd Markdown\n",
        "df[\"video_error_or_removed\"].value_counts()\n",
        "add Codeadd Markdown\n"
      ],
      "metadata": {
        "id": "yOiN5rbWJx9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that out of videos that appeared on trending list (40949 videos), there is a tiny portion (23 videos) with errors.\n",
        "\n",
        "add Codeadd Markdown\n",
        "How many trending videos have their commets disabled?\n",
        "To know that, we use comments_disabled column\n"
      ],
      "metadata": {
        "id": "wyVctiTjJsS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = df[\"comments_disabled\"].value_counts().to_dict()\n",
        "fig, ax = plt.subplots()\n",
        "_ = ax.pie(x=[value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n",
        "           colors=['#003f5c', '#ffa600'], textprops={'color': '#040204'})\n",
        "_ = ax.axis('equal')\n",
        "_ = ax.set_title('Comments Disabled?')\n",
        "df[\"comments_disabled\"].value_counts(normalize=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "r3lEcA3_JmqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that only 2% of trending videos prevented users from commenting.\n",
        "\n",
        "How many trending videos have their ratings disabled?\n",
        "To know that, we use ratings_disabled column\n"
      ],
      "metadata": {
        "id": "CmIavQTNJeFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = df[\"ratings_disabled\"].value_counts().to_dict()\n",
        "fig, ax = plt.subplots()\n",
        "_ = ax.pie([value_counts[False], value_counts[True]], labels=['No', 'Yes'], \n",
        "            colors=['#003f5c', '#ffa600'], textprops={'color': '#040204'})\n",
        "_ = ax.axis('equal')\n",
        "_ = ax.set_title('Ratings Disabled?')\n"
      ],
      "metadata": {
        "id": "zYTwczNxJZ-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"ratings_disabled\"].value_counts()\n"
      ],
      "metadata": {
        "id": "ALex6NBtJVzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "add Codeadd Markdown\n",
        "We see that only 169 trending videos out of 40949 prevented users from commenting.\n",
        "\n",
        "How many videos have both comments and ratings disabled?\n"
      ],
      "metadata": {
        "id": "Icq3oX-gJRdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df[(df[\"comments_disabled\"] == True) & (df[\"ratings_disabled\"] == True)].index)\n"
      ],
      "metadata": {
        "id": "1J-kQOnWJKq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So there are just 106 trending videos that have both comments and ratings disabled\n",
        "\n",
        "Conclusions\n",
        "Here are the some of the results we extracted from the analysis:\n",
        "\n",
        "We analyzed a dataset that contains information about YouTube trending videos for 205 days. The dataset was collected in 2017 and 2018. It contains 40949 video entry.\n",
        "71% of trending videos have less than 1.5 million views, and 91% have less than 5 million views.\n",
        "68% of trending videos have less than 40,000 likes, and 84% have less than 100,000 likes.\n",
        "67% of trending videos have less than 4,000 comments, and 93% have less than 25,000 comments.\n",
        "Some videos may appear on the trending videos list on more than one day. Our dataset contains 40494 entries but not for 40494 unique videos but for 6351unique videos.\n",
        "Trending videos that have 100,000,000 views and more have title length between 33 and 55 characters approximately.\n",
        "The delimiters - and | were common in trending video titles.\n",
        "The words 'Official', 'Video', 'Trailer', 'How', and '2018' were common also in trending video titles.\n",
        "There is a strong positive correlation between the number of views and the number of likes of trending videos: As one of them increases, the other increases, and vice versa.\n",
        "There is a strong positive correlation also between the number of likes and the number of comments, and a slightly weaker one between the number of dislikes and the number of comments.\n",
        "The category that has the largest number of trending videos is 'Entertainment' with 9,964 videos, followed by 'Music' category with 6,472 videos, followed by 'Howto & Style' category with 4146 videos.\n",
        "On the opposite side, the category that has the smallest number of trending videos is 'Shows' with 57 videos, followed by 'Nonprofits & Activisim' with 57 videos, followed by 'Autos & Vehicles' with 384 videos."
      ],
      "metadata": {
        "id": "etlJzqXgI9Lz"
      }
    }
  ]
}